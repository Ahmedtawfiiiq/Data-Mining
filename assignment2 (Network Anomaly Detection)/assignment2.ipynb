{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"plpaDAS3Ym8Y"},"outputs":[],"source":["# from zipfile import ZipFile\n","\n","# file_name = \"data.zip\"\n","\n","# with ZipFile(file_name, 'r') as zip:\n","#   zip.extractall()\n","#   print('Done')"]},{"cell_type":"markdown","metadata":{"id":"oihGbDvt0cir"},"source":["# **LIBRARIES**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0yha6U-Sce2T"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_datasets as tfds\n","import numpy as np\n","import random\n","import pandas as pd\n","from sklearn.preprocessing import MinMaxScaler\n","import math\n","from sklearn.cluster import SpectralClustering\n","from numpy import random\n","from sklearn.metrics.pairwise import rbf_kernel\n","from sklearn.preprocessing import normalize\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import pairwise_distances\n","from sklearn.neighbors import kneighbors_graph\n","import networkx as nx\n","from scipy.optimize import linear_sum_assignment"]},{"cell_type":"markdown","metadata":{"id":"EilHFqCzv4w9"},"source":["# **LOAD TRAINING AND TESTING DATASETS FOR KMEANS**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1021146,"status":"ok","timestamp":1680461217395,"user":{"displayName":"Ahmed Alaa","userId":"06349177510690474716"},"user_tz":-120},"id":"8IV6hg7vvxaQ","outputId":"caba2418-c950-4d3a-c44d-28a31fa9d270"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Cardinality: 4898431\n","Test Cardinality: 311029\n"]}],"source":["# datasets = tfds.load('kddcup99', split=['train', 'test'])\n","\n","# # Calculate the cardinalities\n","# train_cardinality = tf.data.experimental.cardinality(datasets[0]).numpy()\n","# test_cardinality = tf.data.experimental.cardinality(datasets[1]).numpy()\n","\n","# print(f'Train Cardinality: {train_cardinality}') # 4,898,431\n","# print(f'Test Cardinality: {test_cardinality}') # 311,029\n","\n","# percent_10 = 494021\n","# training_df = tfds.as_dataframe(datasets[0].shuffle(buffer_size=1024).take(percent_10))\n","# testing_df = tfds.as_dataframe(datasets[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sf2V6FSrbW65"},"outputs":[],"source":["# training_df = training_df.drop_duplicates()\n","# testing_df = testing_df.drop_duplicates()\n","\n","# training_labels = np.unique(training_df['label'].values)\n","# testing_df = testing_df[testing_df.label.isin(training_labels)]\n","\n","# training_data = np.array(training_df.drop(['label'], axis=1).values.tolist())\n","# testing_data = np.array(testing_df.drop(['label'], axis=1).values.tolist())\n","\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","\n","# training_data = scaler.fit_transform(training_data)\n","# testing_data = scaler.fit_transform(testing_data)\n","\n","# true_labels = testing_df['label'].values.tolist()\n","# ks = [7, 15, 23, 31, 45]"]},{"cell_type":"markdown","metadata":{"id":"1WThuIc0yMGJ"},"source":["# **LOAD TRAINING AND TESTING DATASETS FOR SPECTRAL**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qNke-T3yKAP"},"outputs":[],"source":["# datasets = tfds.load('kddcup99', split=['train', 'test'])\n","# samples_per_class = int(4898431 * 0.005 / len(classes))\n","\n","# from functools import reduce\n","# datasets_per_class = []\n","\n","# for i in classes:\n","#     class_dataset = datasets[0].filter(lambda x: x[\"label\"] == i)\n","#     class_dataset = class_dataset.map(lambda x: {k: v for k, v in x.items() if k != 'label'})\n","#     class_dataset = class_dataset.take(samples_per_class)\n","#     datasets_per_class.append(class_dataset)\n","\n","# import csv\n","\n","# for i in range(10, 23):\n","#   data = []\n","#   index = i\n","#   file_name = 'class'+str(index)+'.csv'\n","#   for i, training_tf_class in enumerate(datasets_per_class):\n","#     if i == index:\n","#       data.extend([list(sample.values()) for sample in tfds.as_numpy(training_tf_class.take(samples_per_class))])\n","\n","#   with open(file_name, mode='w') as file:\n","#       writer = csv.writer(file)\n","#       for row in data:\n","#           writer.writerow(row)"]},{"cell_type":"markdown","metadata":{"id":"3lSb_-CF0gfn"},"source":["# **FUNCTIONS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iiF1d95uY0NW"},"outputs":[],"source":["def kmeans_plus_plus(data, k):\n","    centroids = [data[np.random.randint(data.shape[0])]]\n","    for _ in range(k - 1):\n","        distances = np.min(np.sum((data[:, np.newaxis] - centroids) ** 2, axis=2), axis=1)\n","        probabilities = distances / distances.sum()\n","        cumulative_probabilities = probabilities.cumsum()\n","        r = np.random.rand()\n","        i = random.randint(0, len(data))\n","        for j, p in enumerate(cumulative_probabilities):\n","            if r < p:\n","                i = j\n","                break\n","        centroids.append(data[i])\n","    return centroids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QLGKiJkkYyTI"},"outputs":[],"source":["def k_means_clustering(data, init_centroids, k, max_iterations):\n","\n","    centroids = init_centroids\n","    old_centroids = None\n","\n","    iterations = 0\n","    empty_flag = False\n","    while (not compare_centroids(old_centroids, centroids)) and (\n","        iterations < max_iterations\n","    ):\n","        old_centroids = centroids\n","        labels = get_labels(data, centroids)\n","        centroids, flag = get_centroids(data, labels, k)\n","        empty_flag = flag\n","        while empty_flag:\n","          labels = get_labels(data, centroids)\n","          centroids, flag = get_centroids(data, labels, k)\n","          empty_flag = flag\n","        iterations += 1\n","    return centroids\n","\n","\n","def compare_centroids(old_centroids, centroids):\n","    if old_centroids is not None:\n","        return np.array_equal(old_centroids, centroids)\n","    else:\n","        return False\n","\n","\n","def get_labels(data, centroids):\n","    distances = [\n","        [np.linalg.norm(data[j] - centroids[i]) for i in range(len(centroids))]\n","        for j in range(len(data))\n","    ]\n","    return np.argmin(distances, axis=1)\n","\n","\n","def get_centroids(data, labels, k):\n","    empty_flag = False\n","    new_centroids = []\n","    for i in range(k):\n","        cluster = []\n","        for j in range(len(data)):\n","            if labels[j] == i:\n","                cluster.append(data[j])\n","        if len(cluster) > 0:\n","            new_centroid = np.mean(cluster, axis=0).reshape((1, -1)).squeeze()\n","            new_centroids.append(new_centroid.tolist())\n","        # when a centroid is empty\n","        else:\n","            empty_flag = True\n","            while True:\n","                # kmeans++ re-initialize\n","                new_centroid = kmeans_plus_plus(data, 1)\n","                # check existance\n","                if not new_centroid in new_centroids:\n","                    break\n","            new_centroids.append(new_centroid.tolist())\n","    return np.array(new_centroids), empty_flag"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DOIKzSq5zLXx"},"outputs":[],"source":["def get_row_sum(matrix, row_index):\n","    row = matrix[row_index]\n","    return sum(row)\n","\n","\n","def get_col_sum(matrix, column_index):\n","    column = [row[column_index] for row in matrix]\n","    return sum(column)\n","\n","\n","def get_contingency_matrix(clusters_labels, classes):\n","    return np.array([[clusters_labels[i].count(x) for x in classes] for i in range(k)])\n","\n","\n","def get_recall_precision(contingency_matrix):\n","    cluster_max = [max(row) for row in contingency_matrix]\n","\n","    precision = []\n","    recall = []\n","    for i in range(contingency_matrix.shape[0]):\n","        index = np.where(contingency_matrix == cluster_max[i])\n","        index = [index[0][0], index[1][0]]\n","        col_sum = get_col_sum(contingency_matrix, index[1])\n","        row_sum = get_row_sum(contingency_matrix, index[0])\n","        precision.append(cluster_max[i] / row_sum)\n","        recall.append(cluster_max[i] / col_sum)\n","    return recall, precision\n","\n","\n","def get_f1_score(recalls, precisions):\n","    f1s = []\n","    for recall, precision in zip(recalls, precisions):\n","        if recall == 0 or precision == 0:\n","            f1 = 0\n","        else:\n","            f1 = 2 * (precision * recall) / (precision + recall)\n","        f1s.append(f1)\n","    sum = 0\n","    count = 0\n","    for score in f1s:\n","        if score != 0:\n","            sum += score\n","            count += 1\n","    return f1s, sum / count\n","\n","\n","def get_conditional_entropy(contingency_matrix):\n","\n","    cluster_specifics = []\n","    conditional_entropy = 0\n","    sum = contingency_matrix.sum()\n","\n","    for i in range(contingency_matrix.shape[0]):\n","        row_sum = get_row_sum(contingency_matrix, i)\n","        cluster_specific = 0\n","        for j in range(contingency_matrix.shape[1]):\n","            p = contingency_matrix[i][j] / row_sum\n","            if p != 0:\n","                cluster_specific -= p * math.log2(p)\n","        cluster_specifics.append(cluster_specific)\n","\n","    for i, entropy in enumerate(cluster_specifics):\n","        row_sum = get_row_sum(contingency_matrix, i)\n","        p = row_sum / sum\n","        conditional_entropy += p * entropy\n","    return cluster_specifics, conditional_entropy\n","\n","def get_confusion_matrix(contingency_matrix):\n","  # calculate true positive TP\n","  TP = 0\n","  for i in range(contingency_matrix.shape[0]):\n","    for j in range(contingency_matrix.shape[1]):\n","      element = contingency_matrix[i][j]\n","      if element >= 2:\n","        TP += math.comb(element, 2)\n","\n","  # calculate false positive FP\n","  FP = 0\n","  for i in range(contingency_matrix.shape[0]):\n","    row_sum = get_row_sum(contingency_matrix, i)\n","    FP += math.comb(row_sum, 2)\n","  FP -= TP\n","\n","  # calculate false negative FN\n","  FN = 0\n","  cluster_max = [max(row) for row in contingency_matrix]\n","  for i in range(contingency_matrix.shape[0]):\n","    index = np.where(contingency_matrix == cluster_max[i])\n","    index = [index[0][0], index[1][0]]\n","    col_sum = get_col_sum(contingency_matrix, index[1])\n","    FN += math.comb(col_sum, 2)\n","  FN -= TP\n","\n","  # calculate true negative TN\n","  TN = 0\n","  N = math.comb(contingency_matrix.sum(), 2)\n","  TN = N - TP - FP - FN\n","\n","  return np.array([[TP, FP], [FN, TN]])\n","\n","\n","def get_rand_index(confusion_matrix):\n","  return np.trace(confusion_matrix)/confusion_matrix.sum()\n","\n","\n","def get_jaccard_index(confusion_matrix):\n","  return confusion_matrix[0][0]/(confusion_matrix.sum() - confusion_matrix[1][1])\n","\n","\n","def get_precision(confusion_matrix):\n","  return confusion_matrix[0][0]/(confusion_matrix[0][0] + confusion_matrix[0][1])\n","\n","\n","def get_recall(confusion_matrix):\n","  return confusion_matrix[0][0]/(confusion_matrix[0][0] + confusion_matrix[1][0])\n","\n","\n","def get_maximum_matching(contingency_matrix):\n","    row_ind, col_ind = linear_sum_assignment(contingency_matrix, maximize=True)\n","    reordered_matrix = contingency_matrix[row_ind, :][:, col_ind]\n","    return np.trace(reordered_matrix)/contingency_matrix.sum(), reordered_matrix"]},{"cell_type":"markdown","metadata":{"id":"k9XMsujoxu_I"},"source":["# **DATA PRE-PROCESSING FOR KMEANS CLUSTERING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vs2AO7N2xxT-"},"outputs":[],"source":["# training_df = training_df.drop_duplicates()\n","# testing_df = testing_df.drop_duplicates()\n","\n","# training_labels = np.unique(training_df['label'].values)\n","# testing_df = testing_df[testing_df.label.isin(training_labels)]\n","\n","# training_data = np.array(training_df.drop(['label'], axis=1).values.tolist())\n","# testing_data = np.array(testing_df.drop(['label'], axis=1).values.tolist())\n","\n","# scaler = MinMaxScaler(feature_range=(0, 1))\n","\n","# training_data = scaler.fit_transform(training_data)\n","# testing_data = scaler.fit_transform(testing_data)\n","\n","# true_labels = testing_df['label'].values.tolist()\n","# ks = [7, 15, 23, 31, 45]"]},{"cell_type":"markdown","metadata":{"id":"pQ14ViHy0pjB"},"source":["# **DATA PRE-PROCESSING FOR SPECTRAL CLUSTERING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrkdsbqldU4C"},"outputs":[],"source":["classes = [27, 16, 14, 7, 25, 20, 34, 15, 9, 1, 19, 32, 8, 18, 4, 3, 12, 30, 35, 6, 2, 23, 17]\n","samples_per_class = [1064,1064,1064,1064,1064,1064,1020,1064,9,1064,264,979,21,4, 53 ,8 ,7 ,2,20,12,30,10,3]\n","\n","without = [44,1051,1033,604,640,426,893,716,9,511,206,918,19,4,53,8,7,2,20,12,30,10,3]\n","\n","training_data = []\n","true_labels = []\n","for i in range(len(classes)):\n","  data = []\n","  df = pd.read_csv('data/class'+str(i)+'.csv', header=None)\n","  for sample in df.to_numpy():\n","    data.append(sample)\n","  data = [list(x) for x in set(tuple(x) for x in data)]\n","  true_labels.extend([classes[i]] * len(data))\n","  for sample in data:\n","    training_data.append(sample)\n","\n","import random\n","\n","zipped = list(zip(training_data, true_labels))\n","\n","random.seed(42)\n","random.shuffle(zipped)\n","\n","training_data, true_labels = zip(*zipped)\n","\n","scaler = MinMaxScaler(feature_range=(0, 1))\n","training_data = scaler.fit_transform(training_data)"]},{"cell_type":"markdown","metadata":{"id":"h9Dlq75Z0udL"},"source":["# **SPECTRAL CLUSTERING**"]},{"cell_type":"markdown","metadata":{"id":"swPRHw3hqoIq"},"source":["**RBF SIMILARITY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTaLE4yRp3zn"},"outputs":[],"source":["# gamma = 0.5\n","# similarity_matrix = rbf_kernel(training_data, gamma = gamma)"]},{"cell_type":"markdown","metadata":{"id":"0wFfYU2XsCBk"},"source":["**NEAREST NEIGHBORS SIMILARITY**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7YMbiqxaTYwo"},"outputs":[],"source":["from scipy.sparse import csr_matrix\n","from scipy.sparse.csgraph import connected_components\n","\n","def check_adjacency_matrix(data, init_neighbors):\n","  neighbor = init_neighbors\n","  while True:\n","    similarity_matrix = kneighbors_graph(data, n_neighbors=neighbor).toarray()\n","    csr = csr_matrix(similarity_matrix)\n","    n_components, component_labels = connected_components(csr)\n","    print(f'n_components: {n_components} neighbor: {neighbor}')\n","    if n_components == 1:\n","      break\n","    neighbor += 1\n","\n","# check_adjacency_matrix(training_data, 40)\n","\n","similarity_matrix = kneighbors_graph(training_data, n_neighbors=43).toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sKry7Xz4Xvvv"},"outputs":[],"source":["degree_matrix = np.diag(np.sum(similarity_matrix, axis=1))\n","laplacian_matrix = degree_matrix - similarity_matrix\n","asymmetric_laplacian = np.linalg.inv(degree_matrix) @ laplacian_matrix\n","\n","eig_vals, eig_vecs = np.linalg.eigh(asymmetric_laplacian)\n","\n","# sort eigen values in ascending order\n","idxs = eig_vals.argsort()\n","eig_vals = eig_vals[idxs]\n","eig_vecs = eig_vecs[:, idxs]\n","\n","k_vectors = 0\n","for i, val in enumerate(eig_vals):\n","  if val > 0:\n","    k_vectors = i\n","    break\n","\n","U = eig_vecs[:, :k_vectors+1]\n","Y = normalize(U)"]},{"cell_type":"markdown","metadata":{"id":"MfBsfgQdwLqo"},"source":["# **CLUSTERING**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9lYnHe-Y5DV"},"outputs":[],"source":["# kmeans\n","# used_data = training_data\n","\n","# spectral\n","used_data = Y\n","testing_data = Y\n","\n","k = 23\n","init_centroids = kmeans_plus_plus(used_data, k)\n","centroids = k_means_clustering(used_data, init_centroids, k, 10)\n","\n","# find cluster of each sample\n","# indexes identifies sample\n","# value at each index identifies cluster of that sample\n","x = get_labels(testing_data, centroids)"]},{"cell_type":"markdown","metadata":{"id":"basoK1KF00zI"},"source":["# **USING SKLEARN**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7SXsF6JY93yk"},"outputs":[],"source":["# from sklearn.cluster import KMeans\n","\n","# k = 23\n","# kmeans = KMeans(n_clusters=k, max_iter=15, n_init=1)\n","# kmeans.fit(training_data)\n","\n","# x = kmeans.predict(training_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oLJGH-FOlwGR"},"outputs":[],"source":["# from sklearn.cluster import SpectralClustering\n","\n","# k = 23\n","# # clustering = SpectralClustering(n_clusters=k, random_state=0, affinity='nearest_neighbors', n_neighbors=43)\n","# clustering = SpectralClustering(n_clusters=k, random_state=0, gamma=0.5)\n","\n","# # x = clustering.fit(training_data)\n","# x = clustering.fit_predict(training_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lLURLK2psum"},"outputs":[],"source":["# x.affinity_matrix_"]},{"cell_type":"markdown","metadata":{"id":"sIrURsTU041b"},"source":["# **CONTINGENCY MATRIX**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSBjUotlztc-"},"outputs":[],"source":["# get samples at each cluster\n","# testing_clusters variable is formed of a list of lists\n","# each list represent a cluster\n","# contents of each cluster represent index of sample in testing data\n","testing_clusters = []\n","for i in range(k):\n","    testing_clusters.append([])\n","\n","for i in range(k):\n","    for j in range(len(testing_data)):\n","        if x[j] == i:\n","            testing_clusters[i].append(j)\n","\n","# replace samples indexes in each cluster by their true label\n","# pred_labels variable is a list of lists\n","# each list represent a cluster and its contents is the true labels of samples\n","# in that cluster\n","clusters_labels = [[true_labels[i] for i in cluster] for cluster in testing_clusters]\n","\n","classes = np.unique(true_labels).tolist()\n","contingency_matrix = get_contingency_matrix(clusters_labels, classes)"]},{"cell_type":"markdown","metadata":{"id":"OH_Xuk1zm1f5"},"source":["# **SCORES**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gewzcmYEz68o"},"outputs":[],"source":["recall, precision = get_recall_precision(contingency_matrix)\n","f1_scores, f1_score = get_f1_score(recall, precision)\n","max_matching, reordered_matrix = get_maximum_matching(contingency_matrix)\n","specifics, conditional_entropy = get_conditional_entropy(contingency_matrix)\n","confusion_matrix = get_confusion_matrix(contingency_matrix)\n","rand_index = get_rand_index(confusion_matrix)\n","jaccard_index = get_jaccard_index(confusion_matrix)\n","confusion_precision = get_precision(confusion_matrix)\n","confusion_recall = get_recall(confusion_matrix)\n","# print(f\"contingency matrix:\\n {contingency_matrix}\")ass\n","print(f'Precision Mean: {np.mean(precision, axis=0)}')\n","print(f'Recall Mean: {np.mean(recall, axis=0)}')\n","print(f\"F1 score: {f1_score}\")\n","# print(f\"Specifics: {specifics}\")\n","# print(f\"Reordered Matrix:\\n {reordered_matrix}\")\n","print(f\"Maximum Matching: {max_matching}\")\n","print(f\"Conditional Entropy: {conditional_entropy}\")\n","print(f\"Confusion Matrix:\\n {confusion_matrix}\")\n","print(f\"Rand Index: {rand_index}\")\n","print(f\"Jaccard Index: {jaccard_index}\")\n","print(f\"Confusion Precison: {confusion_precision}\")\n","print(f\"Confusion Recall: {confusion_recall}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnSo0vJiXzb4"},"outputs":[],"source":["np.sum(contingency_matrix, axis=1)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
